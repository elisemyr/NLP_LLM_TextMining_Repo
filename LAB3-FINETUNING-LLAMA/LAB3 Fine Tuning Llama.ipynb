{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAL0P1HJTzFY"
   },
   "source": [
    "# Lab 3: Finetune Llama 3.2 on Medical Dataset\n",
    "\n",
    "This notebook implements finetuning of Llama 3.2 on a medical dataset using Hugging Face Transformers and PEFT (Parameter-Efficient Fine-Tuning).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpRVudWuUNuj"
   },
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q transformers datasets peft accelerate bitsandbytes trl torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsQNWTDSUQ6t"
   },
   "source": [
    "## Step 2: Import Libraries and Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"‚úÖ Using device: {device} (Apple Silicon)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"‚úÖ Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"‚ö†Ô∏è Using device: {device} (CPU - training will be slower)\")\n",
    "    print(\"Note: If you have a GPU, make sure CUDA is properly installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTYtZvsWUVwE"
   },
   "source": [
    "## Step 3: Load and Prepare Dataset\n",
    "\n",
    "We'll use the MedMCQA dataset for medical question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MedMCQA dataset\n",
    "dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "print(\"\\nSample example:\")\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nM9dsoBUZuf"
   },
   "source": [
    "## Step 4: Format Dataset for Instruction Tuning\n",
    "\n",
    "We need to format the dataset in a way that the model can learn from. We'll create a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(example):\n",
    "    \"\"\"Format medical Q&A into instruction format\"\"\"\n",
    "    question = example.get(\"question\", \"\")\n",
    "    # Extract options from 'opa', 'opb', 'opc', 'opd'\n",
    "    options = {}\n",
    "    if 'opa' in example and example['opa'] is not None: options['a'] = example['opa']\n",
    "    if 'opb' in example and example['opb'] is not None: options['b'] = example['opb']\n",
    "    if 'opc' in example and example['opc'] is not None: options['c'] = example['opc']\n",
    "    if 'opd' in example and example['opd'] is not None: options['d'] = example['opd']\n",
    "\n",
    "    cop_idx = example.get(\"cop\")  # Correct option index (0 for 'a', 1 for 'b', etc.)\n",
    "\n",
    "    # Build options list\n",
    "    # Ensure options are sorted by key (a, b, c, d)\n",
    "    sorted_option_keys = sorted(options.keys())\n",
    "    options_text = \"\\n\".join([f\"{k.upper()}. {options[k]}\" for k in sorted_option_keys])\n",
    "\n",
    "    # Get correct answer text\n",
    "    correct_answer_text = \"\"\n",
    "    if cop_idx is not None and 0 <= cop_idx < len(sorted_option_keys):\n",
    "        correct_answer_key = sorted_option_keys[cop_idx] # 'a', 'b', 'c', or 'd'\n",
    "        correct_answer_text = options[correct_answer_key]\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"You are a medical expert. Answer the following medical question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{options_text}\n",
    "\n",
    "Answer: {correct_answer_text}\"\"\"\n",
    "\n",
    "    # Return 'text' for training, and also original question, options, and cop for later evaluation\n",
    "    return {\"text\": prompt, \"original_question\": question, \"original_options\": options, \"original_cop\": cop_idx}\n",
    "\n",
    "# Format dataset (using subset for faster training)\n",
    "dataset_size = min(2000, len(dataset))\n",
    "dataset_subset_for_formatting = dataset.select(range(dataset_size)) # Explicitly define the subset used for formatting\n",
    "\n",
    "formatted_dataset = dataset_subset_for_formatting.map(\n",
    "    format_instruction,\n",
    "    # Temporarily remove remove_columns to ensure all new columns are added\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset features: {formatted_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2xZ-gRsTVmT"
   },
   "source": [
    "## Local Inference on GPU\n",
    "Model page: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "\n",
    "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkVSbCryTVmU"
   },
   "source": [
    "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "print(\"\\nLoading model...\")\n",
    "# MPS doesn't support float16 well, use float32. CUDA can use float16 for efficiency\n",
    "if torch.backends.mps.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=None,  # MPS doesn't support device_map=\"auto\"\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(device)  # Manually move to MPS device\n",
    "elif torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HqtzbwhTVmV"
   },
   "source": [
    "## Remote Inference via Inference Providers\n",
    "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
    "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
    "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Retrieve the HF_TOKEN from Colab secrets\n",
    "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK04LbHOV7gF"
   },
   "source": [
    "## Step 6: Configure LoRA for Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,  # LoRA alpha\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable input gradients for gradient checkpointing with PEFT\n",
    "# This is crucial when using gradient_checkpointing with PEFT models\n",
    "# to ensure the backward pass works correctly through frozen layers.\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA configuration applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZMIdm0BWBfa"
   },
   "source": [
    "## Step 7: Tokenize Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text examples\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "# Define columns to remove after formatting and tokenization\n",
    "# Keep 'input_ids', 'attention_mask', 'original_question', 'original_options', 'original_cop'\n",
    "columns_to_keep = ['input_ids', 'attention_mask', 'original_question', 'original_options', 'original_cop']\n",
    "\n",
    "# Get all columns in the formatted_dataset\n",
    "all_formatted_columns = formatted_dataset.column_names\n",
    "\n",
    "# Identify columns to remove: all columns except those we want to keep\n",
    "columns_to_remove_after_tokenization = [col for col in all_formatted_columns if col not in columns_to_keep]\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=columns_to_remove_after_tokenization,\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Sample tokenized example keys: {tokenized_dataset[0].keys()}\")\n",
    "print(f\"Tokenized dataset features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCT07PflWIZF"
   },
   "source": [
    "## Step 8: Split Dataset into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset: 80% train, 20% test\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Test examples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m26quYFdWM-D"
   },
   "source": [
    "## Step 9: Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-medical-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Reduced from 4 to 1 for MPS memory\n",
    "    per_device_eval_batch_size=1,  # Reduced from 4 to 1 for MPS memory\n",
    "    gradient_accumulation_steps=16,  # Increased from 4 to 16 to maintain effective batch size (1*16=16)\n",
    "    warmup_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=torch.cuda.is_available(),  # Only enable for CUDA, not MPS\n",
    "    gradient_checkpointing=True,  # Enable to save memory\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",  # Changed from evaluation_strategy to eval_strategy\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DePwO6WhWRF4"
   },
   "source": [
    "## Step 10: Create Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"Data collator created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukj3rT6fWTzh"
   },
   "source": [
    "## Step 11: Initialize Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting training...\")\n",
    "print(\"This may take a while depending on your hardware...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4kiachDXNon"
   },
   "source": [
    "## Step 12: Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./llama-medical-finetuned\")\n",
    "tokenizer.save_pretrained(\"./llama-medical-finetuned\")\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2r2-nw4XPrw"
   },
   "source": [
    "## Step 13 : Evaluation, loading test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test examples from the split tokenized_dataset\n",
    "test_examples = []\n",
    "for i in range(len(test_dataset)):\n",
    "    example = test_dataset[i]\n",
    "    test_examples.append({\n",
    "        \"question\": example[\"original_question\"],\n",
    "        \"options\": example[\"original_options\"],\n",
    "        \"cop\": example[\"original_cop\"]\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(test_examples)} test examples\")\n",
    "print(\"\\nSample test example:\")\n",
    "print(test_examples[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L3vKjitXSay"
   },
   "source": [
    "## Step 14: Create Inference Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, options, model, tokenizer, max_length=512):\n",
    "    \"\"\"Generate an answer for a medical question\"\"\"\n",
    "    #format the prompt\n",
    "    options_text = \"\"\n",
    "    if options:\n",
    "        for key in sorted(options.keys()):\n",
    "            option_label = key.upper()\n",
    "            option_text = options[key]\n",
    "            options_text += f\"{option_label}. {option_text}\\n\"\n",
    "\n",
    "    prompt = f\"\"\" You are a medical expert.Answer the following medical question.\n",
    "\n",
    "Question: {question}\n",
    "Options:\n",
    "{options_text.strip()}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # tokenize\n",
    "    inputs =tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    inputs= {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # generate\n",
    "    with torch.no_grad() :\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7 ,\n",
    "            do_sample=True ,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # decode\n",
    "    generated_text =tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer =generated_text.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    return answer\n",
    "\n",
    "# test the function\n",
    "sample_question =test_examples[0][\"question\"]\n",
    "sample_options= test_examples[0][\"options\"]\n",
    "sample_answer =generate_answer(sample_question, sample_options, model, tokenizer)\n",
    "print(\"Sample generation :\")\n",
    "print(f\"Question:{sample_question}\")\n",
    "print(f\"Generated Answer:{sample_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GdjcPg0XXJG"
   },
   "source": [
    "## Step 15: Implement Accuracy Checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_match(prediction,ground_truth, options):\n",
    "    \"\"\"Check if prediction matches ground truth (exact or partial)\"\"\"\n",
    "    # get ground truth text\n",
    "    if options and isinstance(ground_truth, int):\n",
    "        option_keys = sorted(options.keys())\n",
    "        if 0 <= ground_truth < len(option_keys):\n",
    "            ground_truth_text = options[option_keys[ground_truth]]\n",
    "        else:\n",
    "            return False, \"no_match\"\n",
    "    else:\n",
    "        ground_truth_text = str(ground_truth)\n",
    "\n",
    "    #normalize texts\n",
    "    prediction_lower = prediction.lower().strip()\n",
    "    ground_truth_lower = ground_truth_text.lower().strip()\n",
    "    #check exact match\n",
    "    if ground_truth_lower in prediction_lower or prediction_lower in ground_truth_lower:\n",
    "        return True, \"exact_match\"\n",
    "\n",
    "    #check keywords\n",
    "    ground_truth_words = set(ground_truth_lower.split())\n",
    "    prediction_words = set(prediction_lower.split())\n",
    "    common_words = ground_truth_words.intersection(prediction_words)\n",
    "\n",
    "    if len(common_words) >= len(ground_truth_words) * 0.5:\n",
    "        return True, \"partial_match\"\n",
    "    return False, \"no_match\"\n",
    "\n",
    "#test the function\n",
    "sample_cop = test_examples[0].get(\"cop\")\n",
    "if sample_cop is not None:\n",
    "    match, match_type =check_answer_match(sample_answer, sample_cop, sample_options)\n",
    "    print(f\"Match: {match},Type:{match_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENpPa5qaXZK0"
   },
   "source": [
    "## Step 16: Run Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#evaluate on a subset of test examples\n",
    "eval_size = min(20,len(test_examples))\n",
    "eval_examples =test_examples[:eval_size]\n",
    "\n",
    "results = {\n",
    "    \"exact_matches\": 0 ,\n",
    "    \"partial_matches\": 0,\n",
    "    \"incorrect\": 0,\n",
    "    \"details\": []\n",
    "}\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, example in enumerate(tqdm(eval_examples,desc=\"Evaluating\")):\n",
    "    question = example.get(\"question\", \"\")\n",
    "    options = example.get(\"options\", {})\n",
    "    correct_answer_idx = example.get(\"cop\", None)\n",
    "\n",
    "    #generate answer\n",
    "    prediction = generate_answer(question, options, model, tokenizer)\n",
    "\n",
    "    #check match\n",
    "    if correct_answer_idx is not None :\n",
    "        is_match, match_type = check_answer_match(prediction, correct_answer_idx, options)\n",
    "\n",
    "        # Get ground truth text\n",
    "        option_keys = sorted(options.keys())\n",
    "        if 0 <= correct_answer_idx < len(option_keys):\n",
    "            ground_truth_text = options[option_keys[correct_answer_idx]]\n",
    "        else:\n",
    "            ground_truth_text = f\"Option {correct_answer_idx}\"\n",
    "\n",
    "        if match_type == \"exact_match\":\n",
    "            results[\"exact_matches\"] += 1\n",
    "            status = \"CORRECT\"\n",
    "        elif match_type == \"partial_match\":\n",
    "            results[\"partial_matches\"] += 1\n",
    "            status = \"PARTIAL\"\n",
    "        else:\n",
    "            results[\"incorrect\"] += 1\n",
    "            status = \"INCORRECT \"\n",
    "\n",
    "        results[\"details\"].append({\n",
    "            \"question\": question[:100] + \"...\" if len(question) > 100 else question,\n",
    "            \"ground_truth\": ground_truth_text,\n",
    "            \"prediction\": prediction[:200] + \"...\" if len(prediction) > 200 else prediction,\n",
    "            \"match_type\": match_type,\n",
    "            \"correct\": is_match\n",
    "        })\n",
    "\n",
    "        print(f\"\\n{status}\")\n",
    "        print(f\"Question: {question[:150]}...\")\n",
    "        print(f\"Ground Truth: {ground_truth_text}\")\n",
    "        print(f\"Prediction: {prediction[:150]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "total_time = time.time()- start_time\n",
    "results[\"total_time\"] = total_time\n",
    "results[\"avg_time_per_example\"] = total_time / eval_size\n",
    "\n",
    "print(f\"\\nRunning accuracy: {(results['exact_matches'] +results['partial_matches']) / eval_size * 100:.1f}% ({results['exact_matches'] + results['partial_matches']}/{eval_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_v_z-tsXh2Q"
   },
   "source": [
    "## Step 17: Calculate Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = results[\"exact_matches\"] + results[\"partial_matches\"]\n",
    "total_examples =eval_size\n",
    "exact_match_rate =results[\"exact_matches\"] / total_examples * 100\n",
    "partial_match_rate = results[\"partial_matches\"] / total_examples * 100\n",
    "overall_accuracy = total_correct / total_examples * 100\n",
    "\n",
    "\n",
    "print(\"FINAL RESULTS\")\n",
    "print(f\"\\nTotal examples evaluated: {total_examples}\")\n",
    "print(f\"Exact matches: {results['exact_matches']} ({exact_match_rate:.1f}%)\")\n",
    "print(f\"Partial matches: {results['partial_matches']} ({partial_match_rate:.1f}%)\")\n",
    "print(f\"Total correct: {total_correct} ({overall_accuracy:.1f}%)\")\n",
    "print(f\"Incorrect: {results['incorrect']} ({100 - overall_accuracy:.1f}%)\")\n",
    "print(f\"\\nTotal evaluation time: {total_time / 60:.1f} minutes\")\n",
    "print(f\"Average time per example: {results['avg_time_per_example']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsU_3RDZXkbf"
   },
   "source": [
    "## Step 18: Analyze Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"DETAILED RESULTS\")\n",
    "\n",
    "\n",
    "#separate correct and incorrect examples\n",
    "correct_examples = [r for r in results[\"details\"] if r[\"correct\"]]\n",
    "incorrect_examples = [r for r in results[\"details\"] if not r[\"correct\"]]\n",
    "\n",
    "print(f\"\\nINCORRECT EXAMPLES ({len(incorrect_examples)}):\")\n",
    "\n",
    "for i, ex in enumerate(incorrect_examples[:10], 1):\n",
    "    print(f\"\\n{i}. Question: {ex['question']}\")\n",
    "    print(f\"   Ground Truth: {ex['ground_truth']}\")\n",
    "    print(f\"   Prediction: {ex['prediction']}\")\n",
    "\n",
    "if len(correct_examples) > 0:\n",
    "    print(f\"\\nCORRECT EXAMPLES ({len(correct_examples)}):\")\n",
    "\n",
    "    for i, ex in enumerate(correct_examples[:5], 1):\n",
    "        print(f\"\\n{i}. Question: {ex['question']}\")\n",
    "        print(f\"   Ground Truth: {ex['ground_truth']}\")\n",
    "        print(f\"   Prediction: {ex['prediction']}\")\n",
    "        print(f\"   Match type: {ex['match_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96u9s7DwXoJZ"
   },
   "source": [
    "## Step 19: Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"PERFORMANCE ASSESSMENT\")\n",
    "\n",
    "\n",
    "if overall_accuracy >= 80:\n",
    "    assessment = \"EXCELLENT. Model performs very well.\"\n",
    "    recommendation = \"Model is ready for further testing and potential deployment.\"\n",
    "elif overall_accuracy >= 60:\n",
    "    assessment = \"GOOD. Model shows promise.\"\n",
    "    recommendation = \"Consider more training epochs or data augmentation.\"\n",
    "elif overall_accuracy >= 40:\n",
    "    assessment = \"MODERATE. Model needs improvement.\"\n",
    "    recommendation = \"Increase training data, adjust hyperparameters, or try different architectures.\"\n",
    "else:\n",
    "    assessment = \"VERY POOR. Model barely learned.\"\n",
    "    recommendation = \"Verify data formatting and retrain from scratch.\"\n",
    "\n",
    "print(f\"\\n{assessment}\")\n",
    "print(f\"Recommendation: {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fWCnKvSXqux"
   },
   "source": [
    "## Step 20: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results in a json file\n",
    "results_summary = {\n",
    "    \"total_examples\": total_examples,\n",
    "    \"exact_matches\": results[\"exact_matches\"],\n",
    "    \"partial_matches\": results[\"partial_matches\"],\n",
    "    \"incorrect\": results[\"incorrect\"],\n",
    "    \"exact_match_rate\": exact_match_rate,\n",
    "    \"partial_match_rate\": partial_match_rate,\n",
    "    \"overall_accuracy\": overall_accuracy,\n",
    "    \"evaluation_time\": total_time,\n",
    "    \"avg_time_per_example\": results[\"avg_time_per_example\"],\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"details\": results[\"details\"]\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3r6zw66XwJp"
   },
   "source": [
    "## Part A: Model Improvement Strategies\n",
    "\n",
    "### Question 1: Improving Model Performance\n",
    "\n",
    "Based on your evaluation results, propose at least 2 or 3 specific strategies to improve your model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgNVXc0rXzDB"
   },
   "source": [
    "1. Increase training dataset size from 2000 to 5000/10000 for more diverse medical scenarios\n",
    "\n",
    "2. adjust Lora parameters, as increasing rank to 32 or 64 for the model to learn more complex patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX_6ln2wX0k-"
   },
   "source": [
    "### Question 2: Analyzing Failure Patterns\n",
    "\n",
    "Review your incorrect predictions and identify patterns in failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"FAILURE PATTERN ANALYSIS\")\n",
    "\n",
    "failure_analysis = {\n",
    "    \"too_verbose\": [],\n",
    "    \"wrong_concept\": [],\n",
    "    \"partial_understanding\": [],\n",
    "    \"hallucination\": []\n",
    "}\n",
    "\n",
    "for ex in incorrect_examples:\n",
    "    pred = ex[\"prediction\"].lower()\n",
    "    gt = ex[\"ground_truth\"].lower()\n",
    "\n",
    "    # verbose answers\n",
    "    if len(ex[\"prediction\"]) > len(ex[\"ground_truth\"]) * 3:\n",
    "        failure_analysis[\"too_verbose\"].append(ex)\n",
    "    # different medical words\n",
    "    elif not any(word in pred for word in gt.split()[:3]):\n",
    "        failure_analysis[\"wrong_concept\"].append(ex)\n",
    "    # partial understanding\n",
    "    elif any(word in pred for word in gt.split()):\n",
    "        failure_analysis[\"partial_understanding\"].append(ex)\n",
    "    else:\n",
    "        failure_analysis[\"hallucination\"].append(ex)\n",
    "print(f\"verbose answers: {len(failure_analysis['too_verbose'])}\")\n",
    "print(f\"different medical words: {len(failure_analysis['wrong_concept'])}\")\n",
    "print(f\"partial understanding: {len(failure_analysis['partial_understanding'])}\")\n",
    "print(f\"hallucination {len(failure_analysis['hallucination'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxGH1CxKX6pQ"
   },
   "source": [
    "### Question 3: Data Quality vs. Quantity\n",
    "\n",
    "What do you think is better between training on 2000 examples (same quality) or 500 curated high-quality examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVlbtuxwX8By"
   },
   "source": [
    "***500 curated high-quality examples as low quality examples can teach the model incorrect patterns, and 500 is still diverse with clean data***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioBZlQRAX96n"
   },
   "source": [
    "## Part B: Resource-Constrained Inference\n",
    "\n",
    "### Question 4: Optimizing for Limited Resources\n",
    "\n",
    "How can you design a strategy to reduce inference time/memory for deployment in constrained environments?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4TGdCKkX_be"
   },
   "source": [
    "- Use 8bit or 4bit to reduce memory\n",
    "- Model pruning\n",
    "- Switch from 3B to 1B or even smaller variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t75qx0kvYA24"
   },
   "source": [
    "### Question 5: Speed vs. Accuracy Trade-offs\n",
    "\n",
    "Analyze how changing generation parameters affects speed, quality, and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different generation parameters\n",
    "import time\n",
    "test_question = test_examples[0][\"question\"]\n",
    "test_options = test_examples[0][\"options\"]\n",
    "\n",
    "configs = [\n",
    "    {\"max_new_tokens\": 50, \"temperature\": 0.1, \"do_sample\": False, \"name\": \"Greedy, Short\"},\n",
    "    {\"max_new_tokens\": 100, \"temperature\": 0.7,\"do_sample\": True, \"name\": \"Sampling, Medium\"},\n",
    "    {\"max_new_tokens\": 200, \"temperature\": 1.0,\"do_sample\": True, \"name\": \"Sampling, Long, High Temp\"},\n",
    "]\n",
    "\n",
    "print(\"SPEED vs ACCURACY ANALYSIS\")\n",
    "\n",
    "for config in configs:\n",
    "    start = time.time()\n",
    "    # generate with different configs\n",
    "    inputs = tokenizer(f\"Question: {test_question}\\nAnswer:\", return_tensors=\"pt\")\n",
    "    inputs ={k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config[\"max_new_tokens\"],\n",
    "            temperature=config[\"temperature\"] ,\n",
    "            do_sample=config[\"do_sample\"] ,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    elapsed =time.time() -start\n",
    "    answer= tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"Time: {elapsed:.3f}s\")\n",
    "    print(f\"Length: {len(answer)} chars\")\n",
    "    print(f\"Answer: {answer[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abi4Oy2zYCgG"
   },
   "source": [
    "\n",
    "\n",
    "- Lower temperature (0.1-0.5): More focused, consistent answers, faster\n",
    "- Higher temperature (0.7-1.0): More diverse but potentially less accurate, slower¬£\n",
    "- Shorter max_new_tokens: Faster inference, but may truncate answers\n",
    "- Longer max_new_tokens: Slower, but allows complete answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xciMGanEYJiF"
   },
   "source": [
    "## Part C: Evaluation Methodology\n",
    "\n",
    "### Question 7: Improving Evaluation Metrics\n",
    "\n",
    "Analyze limitations of current exact/partial match evaluation and propose improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRFB3VgkYK1K"
   },
   "source": [
    "Limitations:\n",
    "\n",
    "1. False Negatives : Model may give correct answer but in different wording\n",
    "2. False Positives. : Partial matches might accept incorrect\n",
    "3. No Semantic Understanding: Doesn't use embeddings to check meaning similarity\n",
    "4. No Medical Terms Handling: Doesn't account for synonyms\n",
    "\n",
    "Proposed Improvements:\n",
    "1. Semantic similarity\n",
    "2. Medical NER\n",
    "3. Human evaluation\n",
    "4. Confidence scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69GkGVz2YMzM"
   },
   "source": [
    "### Question 8: Test Set Size and Confidence\n",
    "\n",
    "Test other test sizes and observe the results. What can you say about the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_sizes = [10, 20, 50, 100]\n",
    "\n",
    "print(\"TEST SET SIZE ANALYSIS\")\n",
    "\n",
    "\n",
    "for size in test_sizes:\n",
    "    if size >len(test_examples):\n",
    "        continue\n",
    "\n",
    "    eval_subset =test_examples[:size]\n",
    "    correct =0\n",
    "\n",
    "    for example in eval_subset:\n",
    "        question =example.get(\"question\", \"\")\n",
    "        options =example.get(\"options\", {})\n",
    "        correct_answer_idx = example.get(\"cop\", None)\n",
    "\n",
    "        prediction =generate_answer(question, options, model, tokenizer)\n",
    "\n",
    "        if correct_answer_idx is not None:\n",
    "            is_match, _ = check_answer_match(prediction, correct_answer_idx, options)\n",
    "            if is_match:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / size * 100\n",
    "    print(f\"Test size: {size:3d} | Accuracy: {accuracy:5.1f}% | Correct: {correct}/{size}\")\n",
    "\n",
    "print(\"larger test sets provide more reliable estimates\")\n",
    "print(\"Small test sets have high variance\")\n",
    "print(\"Need at least 50-100 examples for a accurate result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMoQuaP_YPfb"
   },
   "source": [
    "## Part D: Real-World Deployment Scenario\n",
    "\n",
    "### Question 9: Production Considerations\n",
    "\n",
    "What can you do to address safety, reliability, updates, and edge cases for deploying in a medical assistance application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65igpfjeYQ9p"
   },
   "source": [
    "\n",
    "1. Safety:\n",
    "   - Add disclaimers\n",
    "   - Implement confidence thresholds\n",
    "   - Input validation\n",
    "\n",
    "2. Reliability\n",
    "   - Track accuracy, latency, error rates\n",
    "   - Redundancy\n",
    "   - Error handling\n",
    "\n",
    "3. Updates\n",
    "   - Track model versions and performance\n",
    "   - A/B testing between models before starting\n",
    "   - Quick revert to previous model version\n",
    "\n",
    "4. Edge Cases\n",
    "   - Out-of-domain questions\n",
    "   - Ambiguous questions\n",
    "   - Handle different languages\n",
    "   - Context length limits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
